{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15c4cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path to the 'schema' directory\n",
    "schema_dir = os.path.join(os.getcwd(), 'schema')\n",
    "\n",
    "# Add this directory to sys.path\n",
    "if schema_dir not in sys.path:\n",
    "    sys.path.append(schema_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25661bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "import json\n",
    "import base64\n",
    "\n",
    "from schema import Tracing_pb2\n",
    "import Common_pb2\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70323e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675fcb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e31383bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_protobuf_to_dict(pb_obj):\n",
    "    result = {}\n",
    "    for field in pb_obj.DESCRIPTOR.fields:\n",
    "        value = getattr(pb_obj, field.name)\n",
    "        if field.type == field.TYPE_MESSAGE:  # Nested protobuf\n",
    "            if field.label == field.LABEL_REPEATED:\n",
    "                result[field.name] = [convert_protobuf_to_dict(item) for item in value]\n",
    "            else:\n",
    "                result[field.name] = convert_protobuf_to_dict(value)\n",
    "        elif field.type == field.TYPE_ENUM:\n",
    "            result[field.name] = field.enum_type.values_by_number.get(value).name\n",
    "        else:\n",
    "            result[field.name] = value\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ccfad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def parse_binary_data(binary_data):\n",
    "    trace_segment_object = Tracing_pb2.SegmentObject()\n",
    "    try:\n",
    "        trace_segment_object.ParseFromString(binary_data)\n",
    "        return trace_segment_object\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing binary data:\", e)\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c258a668",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_trace_data_to_csv_row(document):\n",
    "    binary_data = base64.b64decode(document['_source']['data_binary'])\n",
    "    parsed_data = parse_binary_data(binary_data)\n",
    "    parsed_data = convert_protobuf_to_dict(parsed_data)\n",
    "\n",
    "    lst = []\n",
    "    for span in parsed_data['spans']:\n",
    "        lst.append({\n",
    "            'traceId': parsed_data['traceId'],\n",
    "            'traceSegmentId': parsed_data['traceSegmentId'],\n",
    "            'startTime': span['startTime'],\n",
    "            'parentSpanId': span['parentSpanId'],\n",
    "            'spanId': span['spanId'],\n",
    "            'endTime': span['endTime'],\n",
    "            'operationName': span['operationName'],\n",
    "            'peer': span['peer'],\n",
    "            'spanType': span['spanType'],\n",
    "            'spanLayer': span['spanLayer'],\n",
    "            'componentId': span['componentId'],\n",
    "            'isError': span['isError'],\n",
    "            'service': parsed_data['service']\n",
    "        })\n",
    "\n",
    "    return lst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf1921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ###\n",
    "# # This method contains logic to fetch the documents from es\n",
    "# # This method get the documents ingested in last 1m and scrolls\n",
    "# # till all the documents fetched beyond the fault size 10000\n",
    "# ##\n",
    "# def get_data_from_index(index_name, query=None, size=10000, scroll_time='1m'):\n",
    "#     if query is None:\n",
    "#         query = {\"query\": {\"match_all\": {}}}\n",
    "\n",
    "#     # Initialize the scroll\n",
    "#     page = ELASTIC_SEARCH_HOST.search(index=index_name, body=query, scroll=scroll_time, size=size)\n",
    "#     scroll_id = page['_scroll_id']\n",
    "#     hits = page['hits']['hits']\n",
    "\n",
    "#     # Start scrolling\n",
    "#     while len(page['hits']['hits']):\n",
    "#         page = ELASTIC_SEARCH_HOST.scroll(scroll_id=scroll_id, scroll=scroll_time)\n",
    "#         scroll_id = page['_scroll_id']\n",
    "#         hits.extend(page['hits']['hits'])\n",
    "\n",
    "#     # Clear the scroll when done\n",
    "#     ELASTIC_SEARCH_HOST.clear_scroll(scroll_id=scroll_id)\n",
    "\n",
    "#     return hits\n",
    "\n",
    "\n",
    "###\n",
    "# This method aggregate each line of the log data recived from es\n",
    "#\n",
    "##\n",
    "def save_log_data_in_file(data,filename):\n",
    "    log_rows = []\n",
    "    for doc in data:\n",
    "        if doc:\n",
    "            log_rows.append(doc['_source']['content'])\n",
    "\n",
    "    with open(f\"parsed_data\\\\{filename}\", 'w', encoding='utf-8') as file:\n",
    "        for entry in log_rows:\n",
    "            file.write(entry)\n",
    "\n",
    "\n",
    "###\n",
    "# This method aggregate  each line of the trace data recived from es\n",
    "#\n",
    "##\n",
    "def save_trace_data_in_file(data,filename):\n",
    "    csv_rows = []\n",
    "    for doc in data:\n",
    "        row = parse_trace_data_to_csv_row(doc)\n",
    "        if row:\n",
    "            csv_rows.extend(row)\n",
    "\n",
    "    df = pd.DataFrame(csv_rows)\n",
    "    df.to_csv(f\"parsed_data\\\\{filename}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81311764",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_file(filename):\n",
    "    hits = []\n",
    "\n",
    "    with open(f\"..\\\\Analysis\\\\data\\\\RawData\\\\dump\\\\{filename}\", 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                hits.append(data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON line: {e}\")\n",
    "    \n",
    "    print(f\"Total length of data {len(hits)}\")\n",
    "    \n",
    "    return hits\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5117194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of data 349289\n",
      "Total length of data 41148\n"
     ]
    }
   ],
   "source": [
    "save_log_data_in_file(read_file(\"logdata.json\"),\"logdata_no_fault.log\")\n",
    "save_log_data_in_file(read_file(\"logdata_f1.json\"),\"logdata_f1_fault.log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8995f8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of data 142822\n",
      "Total length of data 16039\n"
     ]
    }
   ],
   "source": [
    "save_trace_data_in_file(read_file(\"tracedata.json\"),\"tracedata_no_fault.csv\")\n",
    "save_trace_data_in_file(read_file(\"tracedata_f1.json\"),\"tracedata_f1_fault.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7702d7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
